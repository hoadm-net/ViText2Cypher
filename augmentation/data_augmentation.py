#!/usr/bin/env python3
"""
Data Augmentation Pipeline for ViText2Cypher
Th·ª±c hi·ªán vi·ªác l√†m gi√†u d·ªØ li·ªáu b·∫±ng c√°ch d·ªãch t·ª± ƒë·ªông v·ªõi few-shot learning
S·ª≠ d·ª•ng KNN ƒë·ªÉ t√¨m examplars t·ª´ b·∫£n d·ªãch th·ªß c√¥ng
"""

import json
import os
import random
import numpy as np
import argparse
from typing import List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI
from tqdm import tqdm
import time

# Load environment variables
load_dotenv()

class DataAugmentationPipeline:
    def __init__(self, debug: bool = True):
        """Kh·ªüi t·∫°o pipeline v·ªõi c√°c c·∫•u h√¨nh t·ª´ .env"""
        self.debug = debug
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.embedding_model = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')
        self.openai_model = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
        self.max_tokens = int(os.getenv('OPENAI_MAX_TOKENS', '2000'))

        if self.debug:
            print(f"üîß Config loaded:")
            print(f"   API Key: {'***' + self.openai_api_key[-10:] if self.openai_api_key else 'NOT FOUND'}")
            print(f"   Model: {self.openai_model}")
            print(f"   Embedding Model: {self.embedding_model}")

        # Kh·ªüi t·∫°o OpenAI embeddings
        try:
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=self.openai_api_key,
                model=self.embedding_model
            )
            if self.debug:
                print("‚úì OpenAI Embeddings initialized")
        except Exception as e:
            print(f"‚úó Error initializing embeddings: {e}")
            raise

        # Kh·ªüi t·∫°o OpenAI client
        self.client = OpenAI(api_key=self.openai_api_key)

        # D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c load
        self.manual_translation_data = []
        self.train_data = []
        self.manual_embeddings = []

        # Load template t·ª´ file
        self.translation_template = self._load_template()

    def _load_template(self) -> PromptTemplate:
        """Load template t·ª´ file"""
        template_file = '../templates/few_shot_translation.txt'
        try:
            with open(template_file, 'r', encoding='utf-8') as f:
                template_content = f.read()

            if self.debug:
                print(f"‚úì Template loaded from {template_file}")

            return PromptTemplate(
                input_variables=["schema", "examples", "question"],
                template=template_content
            )
        except Exception as e:
            print(f"‚úó Error loading template: {e}")
            print(f"Creating default template...")
            # Fallback template
            default_template = """B·∫°n l√† chuy√™n gia d·ªãch thu·∫≠t chuy√™n d·ªãch c√¢u h·ªèi t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát.

Schema: {schema}

Examples: {examples}

Question: {question}

Vietnamese:"""
            return PromptTemplate(
                input_variables=["schema", "examples", "question"],
                template=default_template
            )

    def load_manual_translation(self, file_path: str = '../data/manual_translation.json'):
        """Load manual translation data - s·ª≠ d·ª•ng cache n·∫øu c√≥"""
        try:
            # Ki·ªÉm tra xem c√≥ file cache v·ªõi embeddings kh√¥ng
            cache_file = file_path.replace('.json', '_with_embeddings.json')
            
            if os.path.exists(cache_file):
                if self.debug:
                    print(f"üîÑ Loading from cache: {cache_file}")
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.manual_translation_data = json.load(f)
                
                # Load embeddings t·ª´ cache
                self.manual_embeddings = [item['embedding_question'] for item in self.manual_translation_data]
                if self.debug:
                    print(f"‚úì Loaded {len(self.manual_translation_data)} samples with embeddings from cache")
                return True
            
            # N·∫øu kh√¥ng c√≥ cache, load file g·ªëc
            with open(file_path, 'r', encoding='utf-8') as f:
                self.manual_translation_data = json.load(f)

            # Ki·ªÉm tra embeddings trong file g·ªëc
            has_embeddings = any('embedding_question' in item for item in self.manual_translation_data)
            if has_embeddings:
                if self.debug:
                    print(f"‚úì ƒê√£ load {len(self.manual_translation_data)} m·∫´u t·ª´ {file_path}")
                # Load embeddings
                self.manual_embeddings = [item['embedding_question'] for item in self.manual_translation_data]
                return True
            else:
                if self.debug:
                    print(f"‚ö†Ô∏è File kh√¥ng c√≥ embeddings - c·∫ßn t·∫°o cache embeddings...")
                    print(f"üí° ƒê·ªÉ ti·∫øt ki·ªám cost, h√£y t·∫°o cache tr∆∞·ªõc:")
                    print(f"   python create_embeddings_cache.py")
                return False

        except Exception as e:
            print(f"‚úó L·ªói khi load {file_path}: {e}")
            return False

    def _create_embeddings_for_manual_data(self) -> bool:
        """T·ª± t·∫°o embeddings cho manual translation data"""
        try:
            if self.debug:
                print("üîÑ ƒêang t·∫°o embeddings cho manual translation data...")
            
            self.manual_embeddings = []
            for i, item in enumerate(self.manual_translation_data):
                if self.debug and i % 100 == 0:
                    print(f"   ƒêang x·ª≠ l√Ω {i}/{len(self.manual_translation_data)}...")
                
                embedding = self.embeddings.embed_query(item['question'])
                self.manual_embeddings.append(embedding)
                item['embedding_question'] = embedding
                
                # Delay nh·ªè ƒë·ªÉ tr√°nh rate limit
                time.sleep(0.1)
            
            if self.debug:
                print(f"‚úì ƒê√£ t·∫°o {len(self.manual_embeddings)} embeddings")
            return True
            
        except Exception as e:
            print(f"‚úó L·ªói t·∫°o embeddings: {e}")
            return False

    def load_data(self):
        """Load d·ªØ li·ªáu t·ª´ c√°c file JSON"""
        if self.debug:
            print("üîÑ ƒêang load d·ªØ li·ªáu...")

        # Load manual translation v·ªõi embeddings
        if not self.load_manual_translation():
            print("‚ùå Kh√¥ng th·ªÉ load manual translation data")
            return False

        # Hi·ªÉn th·ªã c·∫•u tr√∫c
        if self.debug and self.manual_translation_data:
            sample = self.manual_translation_data[0]
            print(f"   C·∫•u tr√∫c m·∫´u: {list(sample.keys())}")

            # Ki·ªÉm tra tr∆∞·ªùng translation
            translation_field = None
            if 'translation' in sample:
                translation_field = 'translation'
            elif 'question_vi' in sample:
                translation_field = 'question_vi'

            if translation_field:
                print(f"   Tr∆∞·ªùng d·ªãch: '{translation_field}'")
            else:
                print("   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tr∆∞·ªùng d·ªãch")

        # Load train data
        try:
            with open('../data/train.json', 'r', encoding='utf-8') as f:
                self.train_data = json.load(f)
            if self.debug:
                print(f"‚úì ƒê√£ load {len(self.train_data)} m·∫´u t·ª´ train.json")
        except Exception as e:
            print(f"‚úó L·ªói load train.json: {e}")
            return False

        return True

    def find_similar_examples(self, target_question: str, k: int = 2) -> List[Dict[str, Any]]:
        """T√¨m k m·∫´u t∆∞∆°ng t·ª± nh·∫•t b·∫±ng KNN v·ªõi cosine similarity"""
        try:
            # T·∫°o embedding cho c√¢u h·ªèi target
            target_embedding = self.embeddings.embed_query(target_question)

            # T√≠nh similarity v·ªõi t·∫•t c·∫£ embeddings
            similarities = []
            for i, manual_embedding in enumerate(self.manual_embeddings):
                similarity = cosine_similarity(
                    [target_embedding],
                    [manual_embedding]
                )[0][0]
                similarities.append((i, similarity))

            # S·∫Øp x·∫øp theo ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·∫£m d·∫ßn
            similarities.sort(key=lambda x: x[1], reverse=True)

            # L·∫•y k m·∫´u t∆∞∆°ng t·ª± nh·∫•t
            top_k_indices = [idx for idx, _ in similarities[:k]]
            examples = [self.manual_translation_data[i] for i in top_k_indices]

            if self.debug:
                similarities_scores = [sim for _, sim in similarities[:k]]
                print(f"   üìä Top {k} similarity scores: {similarities_scores}")

            return examples

        except Exception as e:
            if self.debug:
                print(f"‚úó L·ªói t√¨m examples: {e}")
            # Fallback: random examples
            return random.sample(self.manual_translation_data, min(k, len(self.manual_translation_data)))

    def format_examples(self, examples: List[Dict[str, Any]]) -> str:
        """Format examples th√†nh string cho few-shot prompt"""
        example_strings = []

        for i, example in enumerate(examples, 1):
            # L·∫•y translation t·ª´ tr∆∞·ªùng c√≥ s·∫µn
            translation = example.get('translation') or example.get('question_vi', '[C·∫ßn d·ªãch]')

            example_str = f"""
Example {i}:
English: {example['question']}
Vietnamese: {translation}
Schema: {example['schema'][:200]}...
Cypher: {example['cypher']}
"""
            example_strings.append(example_str)

        return "\n".join(example_strings)

    def translate_question(self, question: str, schema: str, examples: List[Dict[str, Any]]) -> str:
        """D·ªãch c√¢u h·ªèi s·ª≠ d·ª•ng few-shot learning"""
        try:
            # Format examples
            examples_text = self.format_examples(examples)

            # T·∫°o prompt t·ª´ template
            prompt = self.translation_template.format(
                schema=schema[:500] + "..." if len(schema) > 500 else schema,  # R√∫t g·ªçn schema
                examples=examples_text,
                question=question
            )

            # G·ªçi OpenAI API
            response = self.client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "B·∫°n l√† m·ªôt chuy√™n gia d·ªãch thu·∫≠t chuy√™n nghi·ªáp."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=0.3
            )

            translation = response.choices[0].message.content.strip()

            # L√†m s·∫°ch k·∫øt qu·∫£
            if translation.startswith("Vietnamese:"):
                translation = translation.replace("Vietnamese:", "").strip()

            return translation

        except Exception as e:
            if self.debug:
                print(f"‚úó L·ªói d·ªãch c√¢u h·ªèi: {e}")
            return f"[L·ªói d·ªãch] {question}"

    def augment_data(self, start_idx: int = 0, end_idx: int = None) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu tƒÉng c∆∞·ªùng b·∫±ng few-shot translation"""
        # T√≠nh s·ªë m·∫´u t·ª´ start v√† end
        if end_idx is None:
            actual_samples = 4000
            print(f"\nüîÑ ƒêang t·∫°o {actual_samples} m·∫´u d·ªØ li·ªáu tƒÉng c∆∞·ªùng...")
            print(f"   üìç B·∫Øt ƒë·∫ßu t·ª´: {start_idx}")
        else:
            actual_samples = end_idx - start_idx
            print(f"\nüîÑ ƒêang t·∫°o {actual_samples} m·∫´u d·ªØ li·ªáu tƒÉng c∆∞·ªùng...")
            print(f"   üìç Ph·∫°m vi: t·ª´ {start_idx} ƒë·∫øn {end_idx - 1}")

        # B∆Ø·ªöC 1: Load t·∫•t c·∫£ c√¢u h·ªèi ƒë√£ t·ªìn t·∫°i (manual + logs.txt)
        existing_questions = self.load_existing_questions()

        augmented_data = []
        used_train_indices = set()
        attempts = 0
        max_attempts = actual_samples * 10  # Tr√°nh v√≤ng l·∫∑p v√¥ t·∫≠n

        # Progress bar v·ªõi tqdm
        with tqdm(total=actual_samples, desc="Translating questions", unit="samples") as pbar:
            while len(augmented_data) < actual_samples and attempts < max_attempts:
                attempts += 1

                # B∆Ø·ªöC 2: Random ch·ªçn 1 m·∫´u t·ª´ train.json
                train_idx = random.randint(0, len(self.train_data) - 1)

                # Skip n·∫øu index ƒë√£ d√πng r·ªìi
                if train_idx in used_train_indices:
                    continue

                train_sample = self.train_data[train_idx]
                train_question = train_sample['question']

                # B∆Ø·ªöC 3: Ki·ªÉm tra c√¢u h·ªèi ƒë√£ t·ªìn t·∫°i ch∆∞a?
                if train_question in existing_questions:
                    used_train_indices.add(train_idx)
                    continue  # Random l·∫°i

                try:
                    # B∆Ø·ªöC 4: T√¨m similar examples b·∫±ng KNN v√† d·ªãch
                    similar_examples = self.find_similar_examples(train_question, k=2)

                    # D·ªãch c√¢u h·ªèi v·ªõi few-shot
                    translation = self.translate_question(
                        question=train_question,
                        schema=train_sample['schema'],
                        examples=similar_examples
                    )

                    # T·∫°o m·∫´u d·ªØ li·ªáu m·ªõi
                    augmented_sample = {
                        'question': train_question,
                        'schema': train_sample['schema'],
                        'cypher': train_sample['cypher'],
                        'translation': translation
                    }

                    augmented_data.append(augmented_sample)
                    used_train_indices.add(train_idx)

                    # Th√™m c√¢u h·ªèi m·ªõi v√†o existing_questions ƒë·ªÉ tr√°nh tr√πng l·∫∑p
                    existing_questions.add(train_question)

                    pbar.update(1)

                    # Delay ng·∫Øn ƒë·ªÉ tr√°nh rate limit
                    time.sleep(0.2)

                except Exception as e:
                    if self.debug:
                        tqdm.write(f"‚úó L·ªói m·∫´u {train_idx}: {str(e)[:50]}...")
                    used_train_indices.add(train_idx)
                    continue

        if attempts >= max_attempts:
            print(f"‚ö†Ô∏è ƒê·∫°t gi·ªõi h·∫°n {max_attempts} l·∫ßn th·ª≠, t·∫°o ƒë∆∞·ª£c {len(augmented_data)}/{actual_samples} m·∫´u")

        print(f"‚úÖ Ho√†n th√†nh: {len(augmented_data)} m·∫´u d·ªØ li·ªáu tƒÉng c∆∞·ªùng")
        return augmented_data

    def save_checkpoint(self, data: List[Dict[str, Any]], checkpoint_file: str):
        """L∆∞u checkpoint"""
        try:
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            if self.debug:
                print(f"‚úó L·ªói l∆∞u checkpoint: {e}")

    def load_checkpoint(self, checkpoint_file: str) -> List[Dict[str, Any]]:
        """Load checkpoint"""
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"‚úì Load checkpoint: {len(data)} m·∫´u t·ª´ {checkpoint_file}")
            return data
        except Exception as e:
            if self.debug:
                print(f"‚ö†Ô∏è Kh√¥ng load ƒë∆∞·ª£c checkpoint {checkpoint_file}: {e}")
            return []

    def save_final_data(self, augmented_data: List[Dict[str, Any]], output_file: str = '../data/augmented_data.json'):
        """L∆∞u CH·ªà d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c d·ªãch - KH√îNG bao g·ªìm d·ªØ li·ªáu manual c≈©"""
        print(f"\nüíæ ƒêang l∆∞u d·ªØ li·ªáu m·ªõi v√†o {output_file}...")

        # CH·ªà l∆∞u d·ªØ li·ªáu augmented_data (d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c d·ªãch)
        # KH√îNG g·ªôp v·ªõi manual_data_clean nh∆∞ tr∆∞·ªõc
        final_data = augmented_data

        # L∆∞u file
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ ƒê√£ l∆∞u {len(final_data)} m·∫´u d·ªØ li·ªáu M·ªöI:")
        print(f"   üìä D·ªØ li·ªáu tƒÉng c∆∞·ªùng (m·ªõi d·ªãch): {len(augmented_data)} m·∫´u")
        print(f"   üìÅ File: {output_file}")
        print(f"   ‚ö†Ô∏è L∆∞u √Ω: File ch·ªâ ch·ª©a d·ªØ li·ªáu M·ªöI, kh√¥ng bao g·ªìm manual_translation.json")

        return output_file

    def run_pipeline(self, start_idx: int = 0, end_idx: int = None, output_file: str = None):
        """Ch·∫°y pipeline ƒë∆°n gi·∫£n - kh√¥ng c√≥ checkpoint/resume"""
        print("üöÄ Data Augmentation Pipeline - Few-Shot Translation")
        print("="*60)

        try:
            # Load d·ªØ li·ªáu
            if not self.load_data():
                return None

            # T·∫°o d·ªØ li·ªáu tƒÉng c∆∞·ªùng
            augmented_data = self.augment_data(start_idx, end_idx)

            if not augmented_data:
                print("‚ùå Kh√¥ng t·∫°o ƒë∆∞·ª£c d·ªØ li·ªáu tƒÉng c∆∞·ªùng")
                return None

            # T·∫°o t√™n file output m·∫∑c ƒë·ªãnh
            if output_file is None:
                if end_idx is None:
                    output_file = f'../data/augmented_data_{start_idx}_{start_idx + 4000}.json'
                else:
                    output_file = f'../data/augmented_data_{start_idx}_{end_idx}.json'

            # L∆∞u k·∫øt qu·∫£ cu·ªëi
            result_file = self.save_final_data(augmented_data, output_file)

            print(f"\nüéâ Ho√†n th√†nh pipeline!")
            print(f"üìÅ K·∫øt qu·∫£: {result_file}")

            return result_file

        except Exception as e:
            print(f"\nüí• L·ªói pipeline: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return None

    def load_existing_questions(self):
        """Load t·∫•t c·∫£ c√¢u h·ªèi ƒë√£ t·ªìn t·∫°i t·ª´ manual_translation v√† logs.txt"""
        existing_questions = set()

        # Load t·ª´ manual_translation_with_embeddings.json
        for item in self.manual_translation_data:
            existing_questions.add(item['question'])

        # Load t·ª´ logs.txt n·∫øu t·ªìn t·∫°i
        logs_file = '../data/logs.txt'
        if os.path.exists(logs_file):
            try:
                with open(logs_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        question = line.strip()
                        if question:
                            existing_questions.add(question)
                if self.debug:
                    print(f"‚úì Load th√™m c√¢u h·ªèi t·ª´ {logs_file}")
            except Exception as e:
                if self.debug:
                    print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c {logs_file}: {e}")

        if self.debug:
            print(f"üîç T·ªïng {len(existing_questions)} c√¢u h·ªèi ƒë√£ t·ªìn t·∫°i (c·∫ßn tr√°nh)")

        return existing_questions

def main():
    parser = argparse.ArgumentParser(description='Data Augmentation Pipeline - Few-Shot Translation')
    parser.add_argument('--start', '-s',
                       type=int, default=0,
                       help='V·ªã tr√≠ b·∫Øt ƒë·∫ßu (default: 0)')
    parser.add_argument('--end', '-e',
                       type=int, default=None,
                       help='V·ªã tr√≠ k·∫øt th√∫c (default: None, s·∫Ω t·∫°o 4000 m·∫´u)')
    parser.add_argument('--output', '-o',
                       type=str, default=None,
                       help='File output (default: ../data/augmented_data_{start}_{end}.json)')
    parser.add_argument('--debug',
                       action='store_true',
                       help='B·∫≠t ch·∫ø ƒë·ªô debug')

    args = parser.parse_args()

    # T√≠nh s·ªë m·∫´u t·ª´ start v√† end
    if args.end is None:
        num_samples = 4000
        end_display = f"{args.start + 4000}"
    else:
        num_samples = args.end - args.start
        end_display = str(args.end)

    print("ü§ñ Data Augmentation Pipeline")
    print("=" * 40)
    print(f"üéØ Samples: {num_samples}")
    print(f"üìç Range: {args.start} -> {end_display}")
    if args.output:
        print(f"üìÅ Output: {args.output}")
    else:
        output_name = f"../data/augmented_data_{args.start}_{end_display}.json"
        print(f"üìÅ Output: {output_name}")
    print("=" * 40)

    # Kh·ªüi t·∫°o v√† ch·∫°y pipeline
    pipeline = DataAugmentationPipeline(debug=args.debug)
    result = pipeline.run_pipeline(
        start_idx=args.start,
        end_idx=args.end,
        output_file=args.output
    )

    if result:
        print(f"\n‚úÖ Success! Result: {result}")
    else:
        print("\n‚ùå Pipeline failed!")
        exit(1)

if __name__ == "__main__":
    main()
